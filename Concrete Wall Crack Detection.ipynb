{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Title: Concrete Wall Crack Detection\n",
    "\n",
    "Author: Pi\n",
    "\n",
    "Date created: 12 March 2021\n",
    "\n",
    "Description: Training an image classifier from to detect cracks on concrete walls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter out corrupted images from the training data\n",
    "\n",
    "import os\n",
    "\n",
    "num_skipped = 0\n",
    "for folder_name in (\"Negative\", \"Positive\"):\n",
    "    folder_path = os.path.join(\"Data\", folder_name)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "\n",
    "        if not is_jfif:\n",
    "            num_skipped += 1\n",
    "            # Delete corrupted image\n",
    "            os.remove(fpath)\n",
    "\n",
    "print(\"Deleted %d images\" % num_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generate dataset\n",
    "\n",
    "image_size = (180, 180)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Data\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Data\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "# The images labelled 1 contains crack \n",
    "# and the ones labelled 0 are smooth\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduce diversity into the image dataset by rotating some of the images\n",
    "    \n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's visualizing the augmented samples\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure dataset for performance\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=32)\n",
    "val_ds = val_ds.prefetch(buffer_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Build a model\n",
    "\n",
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Image augmentation block\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=image_size + (3,), num_classes=2)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model with 2 epochs for temporary results\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\"),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "model.save(\"wallcrackdetection.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Images\n",
      "Rebuilding Grid Image\n",
      "Analyzing Crack for splitted/0_0.jpeg\n",
      "Analyzing Crack for splitted/1_0.jpeg\n",
      "Analyzing Crack for splitted/2_0.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/2_0.jpeg\n",
      "Analyzing Crack for splitted/3_0.jpeg\n",
      "Crack Detected with confidence [99.941025] at splitted/3_0.jpeg\n",
      "Analyzing Crack for splitted/4_0.jpeg\n",
      "Analyzing Crack for splitted/5_0.jpeg\n",
      "Analyzing Crack for splitted/6_0.jpeg\n",
      "Crack Detected with confidence [99.99522] at splitted/6_0.jpeg\n",
      "Analyzing Crack for splitted/7_0.jpeg\n",
      "Analyzing Crack for splitted/8_0.jpeg\n",
      "Analyzing Crack for splitted/9_0.jpeg\n",
      "Analyzing Crack for splitted/10_0.jpeg\n",
      "Analyzing Crack for splitted/11_0.jpeg\n",
      "Analyzing Crack for splitted/0_1.jpeg\n",
      "Analyzing Crack for splitted/1_1.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/1_1.jpeg\n",
      "Analyzing Crack for splitted/2_1.jpeg\n",
      "Analyzing Crack for splitted/3_1.jpeg\n",
      "Analyzing Crack for splitted/4_1.jpeg\n",
      "Analyzing Crack for splitted/5_1.jpeg\n",
      "Analyzing Crack for splitted/6_1.jpeg\n",
      "Crack Detected with confidence [98.874756] at splitted/6_1.jpeg\n",
      "Analyzing Crack for splitted/7_1.jpeg\n",
      "Analyzing Crack for splitted/8_1.jpeg\n",
      "Analyzing Crack for splitted/9_1.jpeg\n",
      "Analyzing Crack for splitted/10_1.jpeg\n",
      "Analyzing Crack for splitted/11_1.jpeg\n",
      "Analyzing Crack for splitted/0_2.jpeg\n",
      "Analyzing Crack for splitted/1_2.jpeg\n",
      "Analyzing Crack for splitted/2_2.jpeg\n",
      "Analyzing Crack for splitted/3_2.jpeg\n",
      "Analyzing Crack for splitted/4_2.jpeg\n",
      "Analyzing Crack for splitted/5_2.jpeg\n",
      "Analyzing Crack for splitted/6_2.jpeg\n",
      "Analyzing Crack for splitted/7_2.jpeg\n",
      "Analyzing Crack for splitted/8_2.jpeg\n",
      "Analyzing Crack for splitted/9_2.jpeg\n",
      "Analyzing Crack for splitted/10_2.jpeg\n",
      "Analyzing Crack for splitted/11_2.jpeg\n",
      "Analyzing Crack for splitted/0_3.jpeg\n",
      "Analyzing Crack for splitted/1_3.jpeg\n",
      "Analyzing Crack for splitted/2_3.jpeg\n",
      "Analyzing Crack for splitted/3_3.jpeg\n",
      "Analyzing Crack for splitted/4_3.jpeg\n",
      "Analyzing Crack for splitted/5_3.jpeg\n",
      "Analyzing Crack for splitted/6_3.jpeg\n",
      "Crack Detected with confidence [99.99426] at splitted/6_3.jpeg\n",
      "Analyzing Crack for splitted/7_3.jpeg\n",
      "Crack Detected with confidence [99.99059] at splitted/7_3.jpeg\n",
      "Analyzing Crack for splitted/8_3.jpeg\n",
      "Analyzing Crack for splitted/9_3.jpeg\n",
      "Analyzing Crack for splitted/10_3.jpeg\n",
      "Analyzing Crack for splitted/11_3.jpeg\n",
      "Analyzing Crack for splitted/0_4.jpeg\n",
      "Analyzing Crack for splitted/1_4.jpeg\n",
      "Analyzing Crack for splitted/2_4.jpeg\n",
      "Analyzing Crack for splitted/3_4.jpeg\n",
      "Analyzing Crack for splitted/4_4.jpeg\n",
      "Analyzing Crack for splitted/5_4.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/5_4.jpeg\n",
      "Analyzing Crack for splitted/6_4.jpeg\n",
      "Analyzing Crack for splitted/7_4.jpeg\n",
      "Analyzing Crack for splitted/8_4.jpeg\n",
      "Analyzing Crack for splitted/9_4.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/9_4.jpeg\n",
      "Analyzing Crack for splitted/10_4.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/10_4.jpeg\n",
      "Analyzing Crack for splitted/11_4.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/11_4.jpeg\n",
      "Analyzing Crack for splitted/0_5.jpeg\n",
      "Analyzing Crack for splitted/1_5.jpeg\n",
      "Analyzing Crack for splitted/2_5.jpeg\n",
      "Crack Detected with confidence [99.99995] at splitted/2_5.jpeg\n",
      "Analyzing Crack for splitted/3_5.jpeg\n",
      "Crack Detected with confidence [99.99215] at splitted/3_5.jpeg\n",
      "Analyzing Crack for splitted/4_5.jpeg\n",
      "Analyzing Crack for splitted/5_5.jpeg\n",
      "Analyzing Crack for splitted/6_5.jpeg\n",
      "Analyzing Crack for splitted/7_5.jpeg\n",
      "Analyzing Crack for splitted/8_5.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/8_5.jpeg\n",
      "Analyzing Crack for splitted/9_5.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/9_5.jpeg\n",
      "Analyzing Crack for splitted/10_5.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/10_5.jpeg\n",
      "Analyzing Crack for splitted/11_5.jpeg\n",
      "Analyzing Crack for splitted/0_6.jpeg\n",
      "Analyzing Crack for splitted/1_6.jpeg\n",
      "Crack Detected with confidence [99.99994] at splitted/1_6.jpeg\n",
      "Analyzing Crack for splitted/2_6.jpeg\n",
      "Analyzing Crack for splitted/3_6.jpeg\n",
      "Analyzing Crack for splitted/4_6.jpeg\n",
      "Analyzing Crack for splitted/5_6.jpeg\n",
      "Analyzing Crack for splitted/6_6.jpeg\n",
      "Analyzing Crack for splitted/7_6.jpeg\n",
      "Crack Detected with confidence [99.99898] at splitted/7_6.jpeg\n",
      "Analyzing Crack for splitted/8_6.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/8_6.jpeg\n",
      "Analyzing Crack for splitted/9_6.jpeg\n",
      "Analyzing Crack for splitted/10_6.jpeg\n",
      "Analyzing Crack for splitted/11_6.jpeg\n",
      "Analyzing Crack for splitted/0_7.jpeg\n",
      "Analyzing Crack for splitted/1_7.jpeg\n",
      "Crack Detected with confidence [99.70465] at splitted/1_7.jpeg\n",
      "Analyzing Crack for splitted/2_7.jpeg\n",
      "Analyzing Crack for splitted/3_7.jpeg\n",
      "Analyzing Crack for splitted/4_7.jpeg\n",
      "Analyzing Crack for splitted/5_7.jpeg\n",
      "Analyzing Crack for splitted/6_7.jpeg\n",
      "Analyzing Crack for splitted/7_7.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/7_7.jpeg\n",
      "Analyzing Crack for splitted/8_7.jpeg\n",
      "Analyzing Crack for splitted/9_7.jpeg\n",
      "Analyzing Crack for splitted/10_7.jpeg\n",
      "Analyzing Crack for splitted/11_7.jpeg\n",
      "Analyzing Crack for splitted/0_8.jpeg\n",
      "Analyzing Crack for splitted/1_8.jpeg\n",
      "Analyzing Crack for splitted/2_8.jpeg\n",
      "Analyzing Crack for splitted/3_8.jpeg\n",
      "Analyzing Crack for splitted/4_8.jpeg\n",
      "Analyzing Crack for splitted/5_8.jpeg\n",
      "Analyzing Crack for splitted/6_8.jpeg\n",
      "Analyzing Crack for splitted/7_8.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/7_8.jpeg\n",
      "Analyzing Crack for splitted/8_8.jpeg\n",
      "Analyzing Crack for splitted/9_8.jpeg\n",
      "Analyzing Crack for splitted/10_8.jpeg\n",
      "Analyzing Crack for splitted/11_8.jpeg\n",
      "Analyzing Crack for splitted/0_9.jpeg\n",
      "Analyzing Crack for splitted/1_9.jpeg\n",
      "Analyzing Crack for splitted/2_9.jpeg\n",
      "Analyzing Crack for splitted/3_9.jpeg\n",
      "Analyzing Crack for splitted/4_9.jpeg\n",
      "Analyzing Crack for splitted/5_9.jpeg\n",
      "Analyzing Crack for splitted/6_9.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/6_9.jpeg\n",
      "Analyzing Crack for splitted/7_9.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/7_9.jpeg\n",
      "Analyzing Crack for splitted/8_9.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/8_9.jpeg\n",
      "Analyzing Crack for splitted/9_9.jpeg\n",
      "Analyzing Crack for splitted/10_9.jpeg\n",
      "Analyzing Crack for splitted/11_9.jpeg\n",
      "Crack Detected with confidence [96.847275] at splitted/11_9.jpeg\n",
      "Analyzing Crack for splitted/0_10.jpeg\n",
      "Analyzing Crack for splitted/1_10.jpeg\n",
      "Analyzing Crack for splitted/2_10.jpeg\n",
      "Analyzing Crack for splitted/3_10.jpeg\n",
      "Analyzing Crack for splitted/4_10.jpeg\n",
      "Analyzing Crack for splitted/5_10.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/5_10.jpeg\n",
      "Analyzing Crack for splitted/6_10.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/6_10.jpeg\n",
      "Analyzing Crack for splitted/7_10.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/7_10.jpeg\n",
      "Analyzing Crack for splitted/8_10.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/8_10.jpeg\n",
      "Analyzing Crack for splitted/9_10.jpeg\n",
      "Analyzing Crack for splitted/10_10.jpeg\n",
      "Crack Detected with confidence [99.99996] at splitted/10_10.jpeg\n",
      "Analyzing Crack for splitted/11_10.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/11_10.jpeg\n",
      "Analyzing Crack for splitted/0_11.jpeg\n",
      "Analyzing Crack for splitted/1_11.jpeg\n",
      "Analyzing Crack for splitted/2_11.jpeg\n",
      "Analyzing Crack for splitted/3_11.jpeg\n",
      "Analyzing Crack for splitted/4_11.jpeg\n",
      "Crack Detected with confidence [99.99901] at splitted/4_11.jpeg\n",
      "Analyzing Crack for splitted/5_11.jpeg\n",
      "Crack Detected with confidence [92.93068] at splitted/5_11.jpeg\n",
      "Analyzing Crack for splitted/6_11.jpeg\n",
      "Analyzing Crack for splitted/7_11.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/7_11.jpeg\n",
      "Analyzing Crack for splitted/8_11.jpeg\n",
      "Crack Detected with confidence [99.99796] at splitted/8_11.jpeg\n",
      "Analyzing Crack for splitted/9_11.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/9_11.jpeg\n",
      "Analyzing Crack for splitted/10_11.jpeg\n",
      "Analyzing Crack for splitted/11_11.jpeg\n",
      "Analyzing Crack for splitted/0_12.jpeg\n",
      "Analyzing Crack for splitted/1_12.jpeg\n",
      "Analyzing Crack for splitted/2_12.jpeg\n",
      "Analyzing Crack for splitted/3_12.jpeg\n",
      "Analyzing Crack for splitted/4_12.jpeg\n",
      "Crack Detected with confidence [100.] at splitted/4_12.jpeg\n",
      "Analyzing Crack for splitted/5_12.jpeg\n",
      "Analyzing Crack for splitted/6_12.jpeg\n",
      "Analyzing Crack for splitted/7_12.jpeg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crack Detected with confidence [100.] at splitted/7_12.jpeg\n",
      "Analyzing Crack for splitted/8_12.jpeg\n",
      "Analyzing Crack for splitted/9_12.jpeg\n",
      "Analyzing Crack for splitted/10_12.jpeg\n",
      "Analyzing Crack for splitted/11_12.jpeg\n",
      "Analyzing Crack for splitted/0_13.jpeg\n",
      "Analyzing Crack for splitted/1_13.jpeg\n",
      "Analyzing Crack for splitted/2_13.jpeg\n",
      "Analyzing Crack for splitted/3_13.jpeg\n"
     ]
    }
   ],
   "source": [
    "# Image Analysis\n",
    "# Code from here to bottom is reusable\n",
    "\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Use a pretrained Model - comment line below if training a new model\n",
    "\n",
    "model = keras.models.load_model('wallcrackdetection.h5')\n",
    "# model.summary()\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "filename_to_analyze = \"download2.jpeg\"\n",
    "tile_resolution = 50\n",
    "marker_width = 5\n",
    "confidence_interval= 90\n",
    "\n",
    "\n",
    "def tile(filename, dir_in, dir_out, d):\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    img = Image.open(os.path.join(dir_in, filename))\n",
    "    w, h = img.size\n",
    "    \n",
    "    grid = list(product(range(0, h-h%d, d), range(0, w-w%d, d)))\n",
    "    for i, j in grid:\n",
    "        box = (j, i, j+d, i+d)\n",
    "        out = os.path.join(dir_out, f'{int(i/d)}_{int(j/d)}{ext}')\n",
    "        img.crop(box).save(out)\n",
    "        # i is vertical column count\n",
    "        # j is horizontal row count\n",
    "        row = int(j/d)\n",
    "        column = int(j/d)\n",
    "        \n",
    "print(\"Splitting Images\")\n",
    "tile(filename_to_analyze,\"input\",\"splitted\",tile_resolution)\n",
    "\n",
    "\n",
    "# Rebuild grid image\n",
    "print(\"Rebuilding Grid Image\")\n",
    "# Config:\n",
    "images_dir = 'splitted'\n",
    "name, ext = os.path.splitext(filename_to_analyze)\n",
    "result_grid_filename = './output/'+name+'_marked.jpg'\n",
    "result_figsize_resolution = 10 # 1 = 100px\n",
    "\n",
    "images_list = os.listdir(images_dir)\n",
    "images_count = len(images_list)\n",
    "if '.DS_Store' in images_list:\n",
    "    images_list.remove('.DS_Store')\n",
    "\n",
    "# get row and column count\n",
    "def get_row(filename, dir_in, dir_out, d):\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    img = Image.open(os.path.join(dir_in, filename))\n",
    "    w, h = img.size\n",
    "    grid = list(product(range(0, h-h%d, d), range(0, w-w%d, d)))\n",
    "    for i, j in grid:\n",
    "        box = (j, i, j+d, i+d)\n",
    "        row = int(j/d)\n",
    "    return row+1\n",
    "def get_column(filename, dir_in, dir_out, d):\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    img = Image.open(os.path.join(dir_in, filename))\n",
    "    w, h = img.size\n",
    "    grid = list(product(range(0, h-h%d, d), range(0, w-w%d, d)))\n",
    "    for i, j in grid:\n",
    "        box = (j, i, j+d, i+d)\n",
    "        column = int(i/d)\n",
    "    return column+1\n",
    "\n",
    "\n",
    "row = get_row(filename_to_analyze,\"input\",\"splitted\",tile_resolution)\n",
    "\n",
    "column = get_column(filename_to_analyze,\"input\",\"splitted\",tile_resolution)\n",
    "\n",
    "\n",
    "# Calculate the grid size:\n",
    "grid_size = math.ceil(math.sqrt(images_count))\n",
    "\n",
    "# Create plt plot:\n",
    "\n",
    "fig, axes = plt.subplots(column,row, figsize=(row*3,column*3))\n",
    "\n",
    "current_file_number = 0\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "while i < row:\n",
    "    while j < column:\n",
    "        x_position = current_file_number % column\n",
    "        y_position = current_file_number // column\n",
    "        name, ext = os.path.splitext(images_list[current_file_number])\n",
    "        # print( str(j) + '_' +str(i)+ext)\n",
    "        \n",
    "        current_filename = images_dir + '/' + str(j) + '_' +str(i)+ext\n",
    "\n",
    "        # analyze all the images in the splitted arrays\n",
    "        print(\"Analyzing Crack for \"+current_filename)\n",
    "        image_size = (180, 180)\n",
    "        img = keras.preprocessing.image.load_img(\n",
    "            current_filename, target_size=image_size\n",
    "        )\n",
    "        img_array = keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "        predictions = model.predict(img_array)\n",
    "        score = predictions[0]\n",
    "\n",
    "        smooth_score = 100 * (1 - score)\n",
    "        crack_score = 100 * score\n",
    "\n",
    "        # print(\"This wall is %.2f percent smooth and %.2f percent cracked.\"% (smooth_score,crack_score))\n",
    "\n",
    "        if(crack_score >= confidence_interval):\n",
    "            print(\"Crack Detected with confidence \"+str(crack_score)+\" at \"+ current_filename) \n",
    "            # Manipulate Image According to crackscore\n",
    "            img = Image.open(current_filename)\n",
    "            with Image.open(current_filename) as im:\n",
    "\n",
    "                draw = ImageDraw.Draw(im)\n",
    "                draw.line((0, 0) + im.size, fill=128,width=marker_width)\n",
    "                draw.line((0, im.size[1], im.size[0], 0), fill=128,width=marker_width)\n",
    "\n",
    "                # write to stdout\n",
    "                im.save(current_filename, \"JPEG\")\n",
    "\n",
    "        \n",
    "        # Analysis Done\n",
    "        \n",
    "        plt_image = plt.imread(current_filename)\n",
    "        axes[x_position, y_position].imshow(plt_image)\n",
    "        axes[x_position, y_position].axis(\"off\")\n",
    "        current_file_number += 1\n",
    "        j+= 1\n",
    "    j=0\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0,hspace=0,wspace=0)\n",
    "plt.savefig(result_grid_filename)\n",
    "\n",
    "# Clear the splitted files after the process\n",
    "dir = \"./splitted/\"\n",
    "for f in os.listdir(dir):\n",
    "    os.remove(os.path.join(dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
